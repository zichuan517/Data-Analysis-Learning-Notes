# 02  逻辑回归

### 1.引例：

·根据贷款人的财务信息判断贷款人是否会违约

·根据邮件的内容判断是否为垃圾邮件

### 2.什么是逻辑回归：

逻辑回归适合处理二分类问题

逻辑回归的输出只能在[0，1]中，表示某件事情会发生的概率

逻辑回归的拟合线：S型曲线

可根据需求自由调整阈值

### 3.怎么寻找最优参数？

线性回归：最小二乘法

逻辑回归：最大似然估计

最大似然估计没有直接的求解公式，要靠数值优化算法

### 4.自变量对因变量的影响

根据p值判断自变量对因变量是否有显著预测作用

exp(x)



# DeepSeek:逻辑回归：

---

一、核心思想：什么是逻辑回归？

**逻辑回归**是一种用于解决**二分类**问题的统计学习方法。它的核心目标是：给定一个输入样本（比如一封邮件、一个病人的各项指标），预测它属于某个类别的**概率**。

- **名字的迷惑性**：虽然叫“回归”，但它做的是“分类”。这是因为它的底层用了与线性回归类似的线性模型，但这个线性模型的结果被映射到了0和1之间，从而表示概率。
- **本质**：它不是直接预测类别，而是预测**类别的概率**。我们通过设定一个阈值（通常是0.5）来最终决定类别。
  - 如果预测概率 ≥ 0.5，则判定为正类。
  - 如果预测概率 < 0.5，则判定为负类。

---

### 二、工作原理：从线性到概率

逻辑回归的工作流程可以分解为三个关键步骤：

#### 第1步：建立线性模型

和线性回归一样，逻辑回归首先计算输入特征的加权和。
`z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b`
其中：

- `x₁, x₂, ... xₙ` 是输入特征（比如邮件的关键词频率、病人的年龄等）。
- `w₁, w₂, ... wₙ` 是每个特征对应的权重（或系数），代表了该特征的重要性。
- `b` 是偏置项（截距）。
- `z` 是一个线性输出值，其范围是 `(-∞, +∞)`。

#### 第2步：通过Sigmoid函数进行“压缩”

线性输出 `z` 的值域是全体实数，无法直接表示概率（概率需要在0到1之间）。因此，我们需要一个函数将 `z` 映射到(0, 1)区间。这个函数就是 **Sigmoid函数**（也叫Logistic函数）。

**Sigmoid函数公式：**
`σ(z) = 1 / (1 + e^{-z})`

**Sigmoid函数的特点：**

- 它的值域是(0, 1)，完美契合概率的定义。
- 它的图像是一个优美的S型曲线。
- 当 `z` 趋近于 `+∞`时，`σ(z)` 无限接近1。
- 当 `z` 趋近于 `-∞`时，`σ(z)` 无限接近0。
- 当 `z = 0` 时，`σ(z) = 0.5`。

经过Sigmoid函数，我们就得到了预测概率：
`P(y=1 | x) = σ(z) = 1 / (1 + e^{-(w·x + b)})`
这个公式表示：在给定输入特征 `x` 的条件下，该样本属于正类（y=1）的概率。

#### 第3步：做出分类决策

得到概率 `P(y=1 | x)` 后，我们设定一个阈值（默认为0.5）：

- 如果 `P(y=1 | x) ≥ 0.5`，预测为1（正类）。
- 如果 `P(y=1 | x) < 0.5`，预测为0（负类）。

这个0.5的阈值对应着线性部分 `z = 0` 的那个决策边界。在实际应用中，可以根据业务需求调整阈值（比如在医疗诊断中，为了不漏掉病人，可以降低阈值以提高召回率）。

---

### 三、如何训练模型？—— 损失函数与优化

模型的学习过程就是找到一组最佳的权重 `w` 和偏置 `b`，使得模型的预测概率尽可能接近真实的标签。

#### 损失函数：交叉熵损失函数

我们不能使用线性回归的均方误差，因为在逻辑回归中，使用均方误差会导致损失函数非凸，有很多局部极小值，难以优化。

逻辑回归使用**交叉熵损失函数**，它对于分类问题非常有效。对于单个样本，其损失函数为：
`L(y, ŷ) = - [y · log(ŷ) + (1 - y) · log(1 - ŷ)]`

其中：

- `y` 是真实标签（0或1）。
- `ŷ` 是预测的概率 `P(y=1 | x)`。

这个函数的设计非常巧妙：

- 当真实标签 `y=1` 时，损失函数变为 `-log(ŷ)`。如果预测概率 `ŷ` 接近1，损失就接近0；如果 `ŷ` 接近0，损失会变得非常大。
- 当真实标签 `y=0` 时，损失函数变为 `-log(1 - ŷ)`。如果预测概率 `ŷ` 接近0，损失就接近0；如果 `ŷ` 接近1，损失会变得非常大。

整个训练集的损失是所有样本损失的平均值。

#### 优化算法：梯度下降

为了最小化这个总损失，我们使用**梯度下降**算法。通过计算损失函数对每个参数（`w` 和 `b`）的梯度（导数），然后沿着梯度反方向（即下降最快的方向）更新参数，逐步逼近最优解。

参数更新公式类似于：
`w_new = w_old - α * (∂L/∂w)`
`b_new = b_old - α * (∂L/∂b)`
其中 `α` 是学习率，控制着每一步更新的幅度。

---

### 四、逻辑回归的优缺点

#### 优点：

1. **简单、高效**：实现简单，计算成本低，对于线性可分或近似线性可分的问题效果很好。
2. **可解释性强**：输出的是概率，而不仅仅是类别，提供了更多信息。模型的参数（权重）有明确的解释：权重的大小代表了特征的重要性，权重的正负代表了特征与正类是正相关还是负相关。
3. **不易过拟合**：特别是当使用L1或L2正则化时，可以有效防止过拟合。
4. **概率输出**：在许多需要概率评估的场景（如风险管理）中非常有用。

#### 缺点：

1. **无法解决非线性问题**：它的决策边界本质上是一条直线（或一个超平面），无法直接处理特征与标签之间存在复杂非线性关系的情况。不过可以通过引入特征的多项式项或交互项来缓解。
2. **对特征工程和多重共线性敏感**：如果特征相关性很高（多重共线性），可能会影响模型的稳定性和解释性。
3. **主要用于二分类**：虽然可以通过扩展（如OvR， Softmax）来处理多分类问题，但核心是为二分类设计的。

---

### 五、总结与应用场景

**逻辑回归**是一个通过Sigmoid函数将线性回归结果映射为概率，从而解决二分类问题的模型。

**经典应用场景包括：**

- **金融领域**：信用卡欺诈检测、贷款违约预测。
- **医疗领域**：根据患者症状预测疾病发生概率。
- **营销领域**：预测用户是否会点击广告、是否会购买商品。
- **自然语言处理**：垃圾邮件识别、情感分析（正面/负面）。

它是机器学习入门必学的经典模型，理解了逻辑回归，就为学习更复杂的神经网络等模型打下了坚实的基础。


# DeepSeek：最大似然估计

---

### 一、核心思想：一个生动的比喻

想象一下，你是一位侦探，发现了一个袋子里有黑白两种球。你进行了一次抽球实验：**有放回地抽取了10次，结果抽到了8次黑球，2次白球**。

现在问题来了：**袋子里黑球的比例最可能是多少？**

- 你会想：“如果袋子里全是白球（黑球比例p=0），那我几乎不可能抽到8次黑球。”
- 你也会想：“如果袋子里黑球白球各一半（p=0.5），那我抽到8次黑球的概率也不高。”
- 你自然而然地会觉得：“**最有可能**的情况是，袋子里的黑球比例就是80%（p=0.8）！因为在这个假设下，我观察到‘8黑2白’这个结果的**可能性是最大的**。”

**这就是最大似然估计的核心思想：**

> 我们已经观察到了一组数据（结果），我们要寻找一个模型参数（原因），使得**在这个参数下，观察到当前这组数据的可能性（似然度）达到最大**。
>
> **基本逻辑是：已经发生的，就是最可能发生的。**

---

### 二、数学定义与步骤

#### 1. 似然函数

- **概率**：在参数 `θ` 已知的情况下，预测观测数据 `X` 发生的可能性。`P(X | θ)`
- **似然**：在观测数据 `X` 已知的情况下，去估计参数 `θ` 的值。`L(θ | X)`

从数学形式上看，似然函数和概率质量函数/密度函数是一模一样的，但**视角完全不同**：

- **概率**是固定参数，看数据。
- **似然**是固定数据，看参数。

对于独立同分布的数据集 `D = {x₁, x₂, ..., xₙ}`，似然函数是所有单个数据点概率的乘积：
`L(θ | D) = P(x₁ | θ) * P(x₂ | θ) * ... * P(xₙ | θ)`

#### 2. 最大似然估计

**最大似然估计**的目标就是找到那个能让似然函数 `L(θ | D)` 取得最大值的参数值 `θ_MLE`：
`θ_MLE = argmax L(θ | D)`

#### 3. 对数似然函数

由于概率通常是很小的数字（0到1之间），连乘会导致结果是一个极其小的数，容易造成计算机浮点数下溢。同时，连乘在数学上也难以处理。

因此，我们通常对似然函数取自然对数，将**连乘变为连加**，得到**对数似然函数**：
`LL(θ | D) = log L(θ | D) = Σ log P(x_i | θ)`

因为对数函数是单调递增函数，所以最大化 `L(θ | D)` 等价于最大化 `LL(θ | D)`。
`θ_MLE = argmax LL(θ | D)`

#### 4. 求解过程

为了找到最大值，我们使用微积分：对对数似然函数关于参数 `θ` 求导，令导数等于零，然后解出 `θ`。
`∂LL(θ | D) / ∂θ = 0`

---

### 三、实例：硬币抛掷（伯努利分布）

假设我们有一枚硬币，抛掷10次，得到数据：`D = {H, T, T, H, H, H, T, H, H, T}` (H代表正面，T代表反面)。我们想估计这枚硬币正面朝上的概率 `p`。

1. **建立模型**：单次抛掷是伯努利试验。`P(H | p) = p`, `P(T | p) = 1 - p`。
2. **写出似然函数**：
   `L(p | D) = P(D | p) = p * (1-p) * (1-p) * p * p * p * (1-p) * p * p * (1-p)`
   简化后：`L(p | D) = p⁶ * (1-p)⁴` (因为10次中有6次正面，4次反面)
3. **写出对数似然函数**：
   `LL(p | D) = log(L(p | D)) = 6 * log(p) + 4 * log(1-p)`
4. **对 `p` 求导并令其为零**：
   `d(LL)/dp = 6/p - 4/(1-p) = 0`
5. **解方程**：
   `6/p = 4/(1-p)`
   `6(1-p) = 4p`
   `6 - 6p = 4p`
   `6 = 10p`
   `p = 6/10 = 0.6`

**结论**：根据最大似然估计，硬币正面朝上的概率 `p` 最可能是 **0.6**。这个结果非常直观，正好是正面次数占总次数的比例。

---

### 四、与逻辑回归的联系

现在回到逻辑回归。在逻辑回归中，我们对每个样本的预测是：
`ŷ_i = P(y_i=1 | x_i) = σ(w·x_i + b)`

我们的真实标签是 `y_i` (0或1)。那么，在给定所有数据 `(X, y)` 的情况下，观察到这一系列 `y_i` 的“概率”或“似然”是多少？

- 对于 `y_i=1` 的样本，我们希望 `ŷ_i` 越大越好。
- 对于 `y_i=0` 的样本，我们希望 `1 - ŷ_i` 越大越好。

我们可以将单个样本的“概率”写为一个统一的公式：
`P(y_i | x_i) = (ŷ_i)^(y_i) * (1 - ŷ_i)^(1 - y_i)`
（你可以验证一下，当y_i=1或0时，这个公式分别变成了ŷ_i和1-ŷ_i）

那么，整个数据集的似然函数就是所有样本概率的乘积：
`L(w, b | X, y) = Π [ŷ_i^(y_i) * (1 - ŷ_i)^(1 - y_i)]`

取对数后，得到对数似然函数：
`LL(w, b) = Σ [y_i * log(ŷ_i) + (1 - y_i) * log(1 - ŷ_i)]`

**注意看，这个式子是不是很眼熟？**

**最大化这个对数似然函数 `LL(w, b)`，完全等价于最小化我们之前提到的交叉熵损失函数 `-LL(w, b)`！**

这就是为什么逻辑回归要用交叉熵损失函数——**它并非凭空而来，而是从最大似然估计的理论中自然推导出来的。**

---

### 五、总结

- **最大似然估计** 是一种寻找模型参数的原则性方法，其理念是 **“已发生的事件最可能发生”**。
- 它的操作是：**寻找能使观测数据出现概率最大的参数值**。
- 通过引入**对数似然**，将复杂的乘积问题转化为更易处理的求和问题。
- 它是许多机器学习模型（尤其是逻辑回归）训练过程的**理论核心**，将“模型训练”这个工程问题与“参数估计”这个统计问题优雅地联系了起来。

理解了MLE，你就能从更深的层面理解为什么逻辑回归的损失函数是那个样子，而不是均方误差。这是一种贯穿机器学习和统计学的强大思想。
